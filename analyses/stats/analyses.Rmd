---
output:
    html_document:
        theme: journal
        highlight: zenburn
---

```{r setup}
library(dplyr)
library(here)
library(cmdstanr)
library(purrr)
library(ggplot2)
library(duckdb)
library(DBI)

m_data <- readRDS(
  here::here(
    "analyses",
    "stats",
    "analyses_model_fit.RData"
  )
)

duck_conn <- DBI::dbConnect(
  drv = duckdb::duckdb(),
  dbdir = here::here(
    "analyses",
    "stats",
    "analyses_model_generated-quantities.duckdb"
  )
)
```

# Checking the sampling behavior

```{r extract_draws_and_summary, cache = TRUE}
d_posterior_draws <- m_data$draws(format = "df")
d_posterior_summary <- m_data$summary()

d_genquant_draws <- generated_quantities$draws(format = "df")
```

Check the range of effective sample sizes:

```{r check_ess}
d_summary %>%
  dplyr::select(
    .,
    ess_bulk,
    ess_tail
  ) %>%
  purrr::map(., range)
```

ESSs just above 10k seem a little low. Examine the distribution:

```{r ess_distributions}
d_summary %>%
  ggplot2::ggplot(
    data = .,
    mapping = ggplot2::aes(
      x = ess_bulk
    )
  ) +
  ggplot2::geom_histogram(
    binwidth = 200
  )
```

Most of the parameters have ESSs well above 75k, which is great. Let's see
which parameters have ESSs less than 25k:

```{r ess_le_25k}
d_summary %>%
  dplyr::filter(
    .,
    ess_bulk < 25000
  ) %>%
  dplyr::select(
    ,
    variable,
    ess_bulk
  ) %>%
  dplyr::arrange(
    .,
    dplyr::desc(ess_bulk)
  ) %>%
  print.data.frame()
```

Most of the parameters are participant-specific deflections from the grand
mean.

# Model inference

We fit a hierarchical Bayesian model using the Stan probabilistic programming
language (REFERENCA) and the `cmdstanr` interface (REFERENCA) for the R
programming language (REFERENCA). The model we fit assumes that each reaction
time \(RT_{i}\) (where \(i \in \{1, \ldots, N\}\) indexes observations to which
we fit the model) comes from a shifted log-normal distribution:

\[
    RT_{i} \sim \textrm{ShiftLogNormal}(\mu_{i}, \sigma, \delta_{P(i)}).
\]

We assumed that each distribution's mean \(\mu_{i}\) depends on the features
of the pertinent word (\(W_{i}\)), as well as participant-specific
(\(\alpha_{P(i)}\)) and word-specific (\(\gamma_{W(i)}\)) factors. As mentioned
earlier, the word features we examined are word length
(\(L_{i}\)), its subjective frequency (\(F_{i}\)), it age-of-acquisition
(\(A_{i}\)), and its concreteness (\(C_{i}\)), with each feature's contribution
presented by a \(\beta\) parameter. All these factors were represented as
deflections from the grand mean \(\alpha_{0}\):

\[
    \mu_i = \alpha_{0} +
        \alpha_{P(i)} +
        L_i \times \beta_L +
        F_i \times \beta_F +
        A_i \times \beta_A +
        C_i \times \beta_C +
        \gamma_{W(i)}.
\]

We assumed a fixed variance of each of the reaction times' distributions
(\(\sigma\)). Finally, to move the values sampled from the log-normal
distributions away from zero (as reaction times cannot realistically be around
zero milliseconds), we assumed a participant-specific shift parameter
\(\delta_{P(i)}\).

The model priors were set so as to try to constrain the values of the
parameters to a range which is plausible given existing research. The complete
procedure for generating the priors is visible in the supplementary materials.

# Model fit

In order to asses model fit, we generated 

```{r closing}
duckdb::duckdb_shutdown(drv = duck_conn)
```

