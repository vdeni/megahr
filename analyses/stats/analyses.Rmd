---
output:
    html_document:
        theme: journal
        highlight: zenburn
---

```{r setup}
library(dplyr)
library(here)
library(cmdstanr)
library(purrr)
library(ggplot2)
library(duckdb)
library(readr)

m_data <- readRDS(
  here::here(
    "analyses",
    "stats",
    "analyses_model_fit.RData"
  )
)

d_observed <- readr::read_delim(
  file = here::here(
    "data",
    "analysis",
    "analysis-data.delim"
  ),
  delim = "|"
)

duck_conn <- DBI::dbConnect(
  drv = duckdb::duckdb(),
  dbdir = here::here(
    "data",
    "analysis",
    "analyses_model_generated-quantities.duckdb"
  ),
  read_only = TRUE
)

DBI::dbExecute(
  conn = duck_conn,
  statement = "SET enable_progress_bar=true"
)
DBI::dbExecute(
  conn = duck_conn,
  statement = "SET threads TO 4"
)
```

# Checking the sampling behavior

```{r extract_draws_and_summary, cache = TRUE}
d_posterior_draws <- m_data$draws(format = "df")
d_posterior_summary <- m_data$summary()
```

Check the range of effective sample sizes:

```{r check_ess}
d_posterior_summary %>%
  dplyr::select(
    .,
    ess_bulk,
    ess_tail
  ) %>%
  purrr::map(., range)
```

ESSs just above 10k seem a little low. Examine the distribution:

```{r ess_distributions}
d_posterior_summary %>%
  ggplot2::ggplot(
    data = .,
    mapping = ggplot2::aes(
      x = ess_bulk
    )
  ) +
  ggplot2::geom_histogram(
    binwidth = 200
  )
```

Most of the parameters have ESSs well above 75k, which is great. Let's see
which parameters have ESSs less than 25k:

```{r ess_le_25k}
d_posterior_summary %>%
  dplyr::filter(
    .,
    ess_bulk < 25000
  ) %>%
  dplyr::select(
    ,
    variable,
    ess_bulk
  ) %>%
  dplyr::arrange(
    .,
    dplyr::desc(ess_bulk)
  ) %>%
  print.data.frame()
```

Most of the parameters are participant-specific deflections from the grand
mean.

# Model inference

We fit a hierarchical Bayesian model using the Stan probabilistic programming
language (REFERENCA) and the `cmdstanr` interface (REFERENCA) for the R
programming language (REFERENCA). The model we fit assumes that each reaction
time \(RT_{i}\) (where \(i \in \{1, \ldots, N\}\) indexes observations to which
we fit the model) comes from a shifted log-normal distribution:

\[
    RT_{i} \sim \textrm{ShiftLogNormal}(\mu_{i}, \sigma, \delta_{P(i)}).
\]

We assumed that each distribution's mean \(\mu_{i}\) depends on the features
of the pertinent word (\(W_{i}\)), as well as participant-specific
(\(\alpha_{P(i)}\)) and word-specific (\(\gamma_{W(i)}\)) factors. As mentioned
earlier, the word features we examined are word length
(\(L_{i}\)), its subjective frequency (\(F_{i}\)), it age-of-acquisition
(\(A_{i}\)), and its concreteness (\(C_{i}\)), with each feature's contribution
presented by a \(\beta\) parameter. All these factors were represented as
deflections from the grand mean \(\alpha_{0}\):

\[
    \mu_i = \alpha_{0} +
        \alpha_{P(i)} +
        L_i \times \beta_L +
        F_i \times \beta_F +
        A_i \times \beta_A +
        C_i \times \beta_C +
        \gamma_{W(i)}.
\]

We assumed a fixed variance of each of the reaction times' distributions
(\(\sigma\)). Finally, to move the values sampled from the log-normal
distributions away from zero (as reaction times cannot realistically be around
zero milliseconds), we assumed a participant-specific shift parameter
\(\delta_{P(i)}\).

The model priors were set so as to try to constrain the values of the
parameters to a range which is plausible given existing research. The complete
procedure for generating the priors is visible in the supplementary materials.

# Model fit

```{r gen_quant_data}
. <- DBI::dbGetQuery(
  conn = duck_conn,
  statement = "
    SELECT max(draw) AS max_draw
    FROM generated_quantities
  "
) %>%
  dplyr::pull(max_draw)

l_gen_quant <- list(
  "n_draws" = .
)
```

In order to asses model fit, we simulated `r l_gen_quant[['n_draws']]` sets of
reaction times from the fitted model, using the obtained posterior parameter
estimates. \ref{fig_ppc_raw_data} shows a random sample of 1000 of the
individual observed reaction times (ordered by magnitude). The shaded area
marks the range which contains the central 90% of the simulated reaction time
value distributions for each data point.

We see that the reaction time values simulated from the fitted model are mostly
compatible with the observed values. The most severe fit issues are visible for
observed values having reaction times above approximately 1 second. This may
not be surprising since such reaction times make up only
`r round(mean(d_observed$stimulus_rt >= 1000) * 100, 2)`% of the complete
dataset, and represent reactions which may be considered outliers (the
longest observed reaction time in the plotted sample was longer than 5
seconds, and its point is not visible in \ref{fig_ppc_raw_data}).

```{r fig.cap="\\label{fig_ppc_raw_data}", dev="png", cache=TRUE}
set.seed(1)
sample_idx <- sample(
  x = 1:nrow(d_observed),
  size = 1000,
  replace = FALSE
) %>%
  sort()


d_observed$rt_id <- 1:nrow(d_observed)
d_observed_sample <- dplyr::filter(
  .data = d_observed,
  rt_id %in% sample_idx
)
d_observed_sample <- dplyr::arrange(
  .data = d_observed_sample,
  stimulus_rt
)

d_rep_summarised <- DBI::dbGetQuery(
  conn = duck_conn,
  statement = "
    SELECT y_rt_rep_id,
        quantile(y_rt_rep_ms, 0.9) AS q90_rt_ms,
        quantile(y_rt_rep_ms, 0.1) AS q10_rt_ms
    FROM generated_quantities
    WHERE y_rt_rep_id IN ( ? )
    GROUP BY y_rt_rep_id
    ORDER BY y_rt_rep_id
  ",
  list(sample_idx)
)
d_rep_summarised <- d_rep_summarised[
  base::match(d_observed_sample$rt_id, d_rep_summarised$y_rt_rep_id),
]

d_rep_summarised$plotting_id <- 1:nrow(d_rep_summarised)
d_observed_sample$plotting_id <- 1:nrow(d_observed_sample)

d_rep_summarised %>%
  ggplot2::ggplot(
    data = .,
    mapping = ggplot2::aes(
      x = plotting_id,
      ymin = q10_rt_ms,
      ymax = q90_rt_ms
    )
  ) +
  ggplot2::geom_ribbon(color = "grey", fill = "grey") +
  ggplot2::geom_point(
    inherit.aes = FALSE,
    data = d_observed_sample,
    mapping = aes(
      x = plotting_id,
      y = stimulus_rt
    ),
    shape = 20
  ) +
  ggplot2::coord_cartesian(
    ylim = c(0, 3000)
  ) +
  ggplot2::scale_y_continuous(
    breaks = seq(0, 3000, 500)
  ) +
  ggplot2::theme(
    panel.grid.major.y = ggplot2::element_line(
      linetype = "dashed",
      color = "black",
      linewidth = 0.1
    )
  ) +
  ggplot2::labs(
    x = "Reaction time ID",
    y = "Reaction time (ms)"
  )
```


```{r closing}
DBI::dbDisconnect(con = duck_conn, shutdown = TRUE)
```

